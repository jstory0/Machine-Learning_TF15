{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d521002b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value : 0.7730317711830139\n",
      "loss value : 0.6941244602203369\n",
      "loss value : 0.6931817531585693\n",
      "loss value : 0.6931490898132324\n",
      "loss value : 0.6931473016738892\n",
      "loss value : 0.6931471824645996\n",
      "loss value : 0.6931471824645996\n",
      "loss value : 0.6931471824645996\n",
      "loss value : 0.6931471824645996\n",
      "loss value : 0.6931471824645996\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression으로 GATE 연산을 학습할 수 있는지 확인!\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Training Data Set\n",
    "x_data = np.array([[0, 0],\n",
    "                   [0, 1],\n",
    "                   [1, 0],\n",
    "                   [1, 1]], dtype=np.float64)\n",
    "\n",
    "# AND GATE 연산에 대한 t_data\n",
    "# t_data = np.array([0, 0, 0, 1], dtype=np.float64)\n",
    "\n",
    "# OR GATE 연산에 대한 t_data\n",
    "# t_data = np.array([0, 1, 1, 1], dtype=np.float64)\n",
    "\n",
    "# XOR GATE 연산에 대한 t_data\n",
    "t_data = np.array([0, 1, 1, 0], dtype=np.float64)\n",
    "\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([2,1]))\n",
    "b = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "# Hypothesis, model\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                              labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(loss)\n",
    "\n",
    "# Session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 반복학습\n",
    "for step in range(30000):\n",
    "    _, loss_val = sess.run([train, loss],\n",
    "                           feed_dict={X:x_data,\n",
    "                                      T:t_data.reshape(-1,1)})\n",
    "    if step % 3000 == 0:\n",
    "        print('loss value : {}'.format(loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faa3ca15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.50      0.40         2\n",
      "         1.0       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.25         4\n",
      "   macro avg       0.17      0.25      0.20         4\n",
      "weighted avg       0.17      0.25      0.20         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluation(모델평가)\n",
    "predict = tf.cast(H >= 0.5, dtype=tf.float32)\n",
    "predict_val = sess.run(predict, feed_dict={X:x_data})\n",
    "print(predict_val)\n",
    "\n",
    "# print(classification_report(정답데이터(1차원), 예측데이터(1차원)))\n",
    "print(classification_report(t_data, predict_val.ravel()))\n",
    "# 결과를 확인해서 logistic regression이 진리표를 학습할 수 있는지를 확인!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8fd3e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value : 0.6976959705352783\n",
      "loss value : 0.6453315615653992\n",
      "loss value : 0.5665238499641418\n",
      "loss value : 0.4159531593322754\n",
      "loss value : 0.22907119989395142\n",
      "loss value : 0.11419809609651566\n",
      "loss value : 0.06408658623695374\n",
      "loss value : 0.0411316379904747\n",
      "loss value : 0.029109908267855644\n",
      "loss value : 0.02204354852437973\n"
     ]
    }
   ],
   "source": [
    "# DNN으로 XOR GATE 연산을 학습할 수 있는지 확인!\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Training Data Set\n",
    "x_data = np.array([[0, 0],\n",
    "                   [0, 1],\n",
    "                   [1, 0],\n",
    "                   [1, 1]], dtype=np.float64)\n",
    "\n",
    "# XOR GATE 연산에 대한 t_data\n",
    "t_data = np.array([0, 1, 1, 0], dtype=np.float64)\n",
    "\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W2 = tf.Variable(tf.random.normal([2,10]))\n",
    "b2 = tf.Variable(tf.random.normal([10]))\n",
    "layer2 = tf.sigmoid(tf.matmul(X,W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random.normal([10,6]))\n",
    "b3 = tf.Variable(tf.random.normal([6]))\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2,W3) + b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random.normal([6,1]))\n",
    "b4 = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "\n",
    "# Hypothesis, model\n",
    "logit = tf.matmul(layer3,W4) + b4\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                              labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(loss)\n",
    "\n",
    "# Session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 반복학습\n",
    "for step in range(30000):\n",
    "    _, loss_val = sess.run([train, loss],\n",
    "                           feed_dict={X:x_data,\n",
    "                                      T:t_data.reshape(-1,1)})\n",
    "    if step % 3000 == 0:\n",
    "        print('loss value : {}'.format(loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6babe7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00         2\n",
      "         1.0       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluation(모델평가)\n",
    "predict = tf.cast(H >= 0.5, dtype=tf.float32)\n",
    "predict_val = sess.run(predict, feed_dict={X:x_data})\n",
    "print(predict_val)\n",
    "\n",
    "# print(classification_report(정답데이터(1차원), 예측데이터(1차원)))\n",
    "print(classification_report(t_data, predict_val.ravel()))\n",
    "# 결과를 확인해서 logistic regression이 진리표를 학습할 수 있는지를 확인!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7217bb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x179d132dc70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensorflow 2.x 버전으로 구현해보자!\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Training Data Set\n",
    "x_data = np.array([[0, 0],\n",
    "                   [0, 1],\n",
    "                   [1, 0],\n",
    "                   [1, 1]], dtype=np.float64)\n",
    "\n",
    "# XOR GATE 연산에 대한 t_data\n",
    "t_data = np.array([0, 1, 1, 0], dtype=np.float64)\n",
    "\n",
    "# Tensorflow 구현\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=(2,)))     # input layer\n",
    "model.add(Dense(units=128,\n",
    "                activation='sigmoid'))   # 1번째 hidden layer\n",
    "model.add(Dense(units=32,\n",
    "                activation='sigmoid'))   # 2번째 hidden layer\n",
    "model.add(Dense(units=16,\n",
    "                activation='sigmoid'))   # 3번째 hidden layer\n",
    "model.add(Dense(units=1,\n",
    "                activation='sigmoid'))   # output layer\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=1e-2),\n",
    "              loss='binary_crossentropy')\n",
    "\n",
    "model.fit(x_data,\n",
    "          t_data.reshape(-1,1),\n",
    "          epochs=30000,\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "469d0190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49988264]\n",
      " [0.5009099 ]\n",
      " [0.49909103]\n",
      " [0.5001188 ]]\n",
      "[0. 1. 0. 1.]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.50      0.50         2\n",
      "         1.0       0.50      0.50      0.50         2\n",
      "\n",
      "    accuracy                           0.50         4\n",
      "   macro avg       0.50      0.50      0.50         4\n",
      "weighted avg       0.50      0.50      0.50         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluation(모델평가)\n",
    "predict_val = model.predict(x_data)\n",
    "print(predict_val)\n",
    "predict_val = tf.cast(predict_val > 0.5, dtype=tf.float32).numpy().ravel()\n",
    "print(predict_val)\n",
    "\n",
    "# print(classification_report(정답데이터(1차원), 예측데이터(1차원)))\n",
    "print(classification_report(t_data, predict_val))\n",
    "# 결과를 확인해서 logistic regression이 진리표를 학습할 수 있는지를 확인!\n",
    "\n",
    "# 파라메타 조절을 더 해야 한다!\n",
    "# 결과가 정상적으로 나와야 한다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machine_TF2]",
   "language": "python",
   "name": "conda-env-machine_TF2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
