{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b802a9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model의 정확도 : 0.9473684210526315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shcho\\anaconda3\\envs\\machine_TF15\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "# 위스콘신 유방암 데이터셋을 이용해서 Logistic Regression을 구현해보자!\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Raw Data Set Loading\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "x_data = cancer.data     # 2차원 ndarray - 독립변수, feature\n",
    "t_data = cancer.target   # 1차원 ndarray - 종속변수, label\n",
    "\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(x_data,\n",
    "                 t_data,\n",
    "                 test_size=0.3,\n",
    "                 stratify=t_data,\n",
    "                 random_state=2)\n",
    "\n",
    "# Model 생성\n",
    "model = linear_model.LogisticRegression()\n",
    "\n",
    "# Model 학습\n",
    "model.fit(train_x_data, train_t_data)\n",
    "\n",
    "# accuracy로 model 평가\n",
    "test_score = model.score(test_x_data, test_t_data)\n",
    "\n",
    "print('Logistic Regression Model의 정확도 : {}'.format(test_score))\n",
    "# 0.9473684210526315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9afd7883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier의 정확도 : 0.8947368421052632\n",
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Raw Data Set Loading\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "x_data = cancer.data     # 2차원 ndarray - 독립변수, feature\n",
    "t_data = cancer.target   # 1차원 ndarray - 종속변수, label\n",
    "\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(x_data,\n",
    "                 t_data,\n",
    "                 test_size=0.3,\n",
    "                 stratify=t_data,\n",
    "                 random_state=2)\n",
    "\n",
    "# Model 생성\n",
    "sgd = linear_model.SGDClassifier(loss='log',   # logistic regression을 이용\n",
    "                                 tol=1e-5,     # loss 값의 차이가 tol값보다 작을 때까지 반복\n",
    "                                 random_state=2)\n",
    "\n",
    "# Model 학습\n",
    "sgd.fit(train_x_data, train_t_data)\n",
    "\n",
    "# Accuracy 측정\n",
    "test_score = sgd.score(test_x_data, test_t_data)\n",
    "\n",
    "print('SGDClassifier의 정확도 : {}'.format(test_score))\n",
    "# 0.8947368421052632\n",
    "# 왜 그럴까...???\n",
    "# 정규화 안했다!  => 각 feature마다 scale이 제각각이다!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "003bb7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화를 이용한 SGDClassifier의 정확도 : 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Raw Data Set Loading\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "x_data = cancer.data     # 2차원 ndarray - 독립변수, feature\n",
    "t_data = cancer.target   # 1차원 ndarray - 종속변수, label\n",
    "\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(x_data,\n",
    "                 t_data,\n",
    "                 test_size=0.3,\n",
    "                 stratify=t_data,\n",
    "                 random_state=2)\n",
    "\n",
    "# Data 정규화\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "# Model 생성\n",
    "sgd = linear_model.SGDClassifier(loss='log',   # logistic regression을 이용\n",
    "                                 tol=1e-5,     # loss 값의 차이가 tol값보다 작을 때까지 반복\n",
    "                                 random_state=2)\n",
    "\n",
    "# Model 학습\n",
    "sgd.fit(scaler.transform(train_x_data), train_t_data)\n",
    "\n",
    "# Accuracy 측정\n",
    "test_score = sgd.score(scaler.transform(test_x_data), test_t_data)\n",
    "\n",
    "print('정규화를 이용한 SGDClassifier의 정확도 : {}'.format(test_score))\n",
    "# 0.9649122807017544\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "285235ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화와 규제를 이용한 SGDClassifier의 정확도 : 0.9707602339181286\n"
     ]
    }
   ],
   "source": [
    "# 위의 코드에 L2 Regularization(규제)도 포함해보자!\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Raw Data Set Loading\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "x_data = cancer.data     # 2차원 ndarray - 독립변수, feature\n",
    "t_data = cancer.target   # 1차원 ndarray - 종속변수, label\n",
    "\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(x_data,\n",
    "                 t_data,\n",
    "                 test_size=0.3,\n",
    "                 stratify=t_data,\n",
    "                 random_state=2)\n",
    "\n",
    "# Data 정규화\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "# Model 생성\n",
    "sgd = linear_model.SGDClassifier(loss='log',   # logistic regression을 이용\n",
    "                                 tol=1e-5,     # loss 값의 차이가 tol값보다 작을 때까지 반복\n",
    "                                 penalty='l2',  # L2 규제를 이용!\n",
    "                                 alpha=0.001,   # 규제 강도\n",
    "                                 random_state=2)\n",
    "\n",
    "# Model 학습\n",
    "sgd.fit(scaler.transform(train_x_data), train_t_data)\n",
    "\n",
    "# Accuracy 측정\n",
    "test_score = sgd.score(scaler.transform(test_x_data), test_t_data)\n",
    "\n",
    "print('정규화와 규제를 이용한 SGDClassifier의 정확도 : {}'.format(test_score))\n",
    "# 0.9707602339181286\n",
    "\n",
    "# 규제를 이용하면 조금 더 나은 모델을 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a3c10dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "(20000, 3)\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "\n",
    "# BMI 예제 구현 - sklearn으로 먼저 구현하고 성능평가를 진행\n",
    "# 성능평가의 metric은 accuracy로 진행\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('../data/bmi.csv', skiprows=3)\n",
    "# display(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5d688577",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 전처리\n",
    "\n",
    "## 결측치 체크부터 해야한다!\n",
    "# print(df.isnull().sum())   # 데이터에 결측치가 없다!\n",
    "\n",
    "## 이상치 체크하고 이상치가 존재하면 처리해야 한다!\n",
    "## z-score 방식으로 알아보자!\n",
    "# zscore_threshold = 2.0\n",
    "\n",
    "# (np.abs(stats.zscore(df['height'])) > zscore_threshold).sum() # => 0이면 이상치가 없다\n",
    "# (np.abs(stats.zscore(df['weight'])) > zscore_threshold).sum() # => 이상치가 없다.\n",
    "# np.unique(df['label'], return_counts=True)   # unique 값의 빈도를 알 수 있다.\n",
    "# 이상치도 없고 데이터의 편향도 존재하지 않는다! 상태가 좋은 데이터이다!\n",
    "\n",
    "# 정규화(Nomalization)를 해야한다!\n",
    "# 일단 먼저 train data와 validation data를 분리한 후 정규화를 진행!\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(df[['height', 'weight']],\n",
    "                 df['label'],\n",
    "                 test_size=0.3,\n",
    "                 random_state=1,\n",
    "                 stratify=df['label'])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "norm_train_x_data = scaler.transform(train_x_data)\n",
    "norm_test_x_data = scaler.transform(test_x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e0895c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn으로 구한 Accuracy : 0.9845\n",
      "[1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shcho\\anaconda3\\envs\\machine_TF15\\lib\\site-packages\\sklearn\\base.py:451: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    }
   ],
   "source": [
    "# Model 생성 후 학습 및 평가\n",
    "model = linear_model.LogisticRegression(C=1000)\n",
    "# 규제를 적용할 수 있다!(L2 규제)\n",
    "# alpha값은 우리가 정해줘야 한다!\n",
    "# C = 1 / alpha\n",
    "\n",
    "model.fit(norm_train_x_data, train_t_data)\n",
    "\n",
    "# 평가를 위한 예측 결과를 얻어낸다!\n",
    "predict_val = model.predict(norm_test_x_data)\n",
    "# 이렇게 나온 예측 결과와 test_t_data를 비교해야 한다!\n",
    "acc = accuracy_score(predict_val, test_t_data)\n",
    "\n",
    "print('sklearn으로 구한 Accuracy : {}'.format(acc))\n",
    "# 0.9851666666666666\n",
    "\n",
    "# predict\n",
    "result = model.predict(scaler.transform(np.array([[187, 81]])))\n",
    "print(result)   # [1] 표준이다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2b5f702a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value : 0.8768877983093262\n",
      "loss value : 0.1644938439130783\n",
      "loss value : 0.12263288348913193\n",
      "loss value : 0.10339578986167908\n",
      "loss value : 0.09187238663434982\n",
      "loss value : 0.08403457701206207\n",
      "loss value : 0.07828311622142792\n",
      "loss value : 0.0738421082496643\n",
      "loss value : 0.0702851265668869\n",
      "loss value : 0.06735719740390778\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow를 이용해서 구현해보자!\n",
    "\n",
    "# multinomial 문제이기 때문에 label을 one-hot encoding 처리 해야한다!\n",
    "# train_t_data, test_t_data를 one-hot encoding으로 변경할건데..\n",
    "# tensorflow의 기능을 이용해서 변경 => tensorflow node로 생성.\n",
    "sess = tf.Session()\n",
    "\n",
    "onehot_train_t_data = sess.run(tf.one_hot(train_t_data, depth=3))   # depth는 class의 개수\n",
    "onehot_test_t_data = sess.run(tf.one_hot(test_t_data, depth=3))   # depth는 class의 개수\n",
    "\n",
    "# tensorflow graph를 그려보자!\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,3], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([2,3]))\n",
    "b = tf.Variable(tf.random.normal([3]))\n",
    "\n",
    "# model, Hypothesis\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit,\n",
    "                                                                 labels=T))\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# session, 초기화\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 반복 학습\n",
    "# 그런데 반복학습할 때 주의해야 할 점이 있다.\n",
    "# 학습데이터의 사이즈가 매우 크면 메모리에 데이터를 한번에 모두 loading할 수 없다.\n",
    "# memory fault나면서 수행이 중지!\n",
    "# 어떻게 해결해야 하나요? => batch 처리를 해야 한다!\n",
    "num_of_epoch = 1000   # 학습을 위한 전체 epoch 수\n",
    "num_of_batch = 100    # 한번에 학습할 데이터 량\n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    total_batch = int(norm_train_x_data.shape[0] / num_of_batch)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_x = norm_train_x_data[i*num_of_batch:(i+1)*num_of_batch]\n",
    "        batch_y = onehot_train_t_data[i*num_of_batch:(i+1)*num_of_batch]\n",
    "        _, loss_val = sess.run([train, loss],\n",
    "                               feed_dict={X: batch_x,\n",
    "                                          T: batch_y})\n",
    "    if step % 100 == 0:\n",
    "        print('loss value : {}'.format(loss_val))\n",
    "\n",
    "# for step in range(10000):\n",
    "#     _, loss_val = sess.run([train, loss], feed_dict={X: norm_train_x_data,\n",
    "#                                                      T: onehot_train_t_data})\n",
    "#     if step % 1000 == 0:\n",
    "#         print('loss value : {}'.format(loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "101d1bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9855\n"
     ]
    }
   ],
   "source": [
    "# 학습이 종료되었다!\n",
    "\n",
    "# 성능평가를 해야한다!\n",
    "# result = sess.run(H, feed_dict={X:scaler.transform(np.array([[187,81]]))})\n",
    "# print(result)\n",
    "# print(np.argmax(result, axis=1))   # 가장 큰 값의 index를 알려준다!\n",
    "\n",
    "predict = tf.argmax(H,1)   # 가장 큰 값의 위치를 찾는다 axis=1\n",
    "correct = tf.equal(predict, tf.argmax(T,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "result = sess.run(accuracy, feed_dict={X: norm_test_x_data,\n",
    "                                       T: onehot_test_t_data})\n",
    "print(result)   # 0.9855"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machine_TF15] *",
   "language": "python",
   "name": "conda-env-machine_TF15-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
